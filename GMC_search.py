import pandas as pd
from naive_search_Novelty import NaiveSearcherNovelty
import test_naive_search_Novelty
from SetSimilaritySearch import SearchIndex
from  process_column import TextProcessor
import pickle
import os
from preprocess_align import gmc_alignmnet_by_query
from preprocess_align import initialize_globally
import csv
import numpy as np
import time
from numpy.linalg import norm
from scipy.spatial import distance
from collections import Counter
import utilities as utl



from SetSimilaritySearch import all_pairs
class GMC_Search:
    """
    GMC_Search: A class for performing Greedy  Marginal  Contribution (GMC) for Novelty based  unionable  table search.
    """

    def __init__(self, data_source, search_parameters=None):
        """
        Initializes the GMC_Search instance.

        :param data_source: The source of the data (e.g., file path, database, etc.).
        :param search_parameters: Dictionary containing parameters for the search (optional).
        """
        self.k=0
        self.data_source = data_source
        self.search_parameters = search_parameters or {}
        self.results = []
        self.data = None
        self.unionability = None
        self.diversity = None
        self.diversity_scores=None
        self.unionability_scores=None
        self.unionable_data=None
        self.alignment_for_diversity_gmc_file=None
        self.query_dl_table_vector_df=None # vector represenation of tables given (query) 
        self.query_s_i_s_j_vector_df=None # vector representation of tables given (query and si)

    def load_data(self):
        """
        Load data from the specified source.

        The schema for the data is expected as:
        ['query_table_name', 'query_column', 'query_column#', 
        'dl_table_name', 'dl_column#', 'dl_column']

        :return: None
        """
        try:
            # Load the CSV file into a pandas DataFrame
            self.data = pd.read_csv(self.data_source)

            # Verify that the required columns are present
            required_columns = ['query_table_name', 'query_column', 'query_column#',
                                'dl_table_name', 'dl_column#', 'dl_column']
            if not all(column in self.data.columns for column in required_columns):
                missing_columns = [col for col in required_columns if col not in self.data.columns]
                raise ValueError(f"Missing required columns in data: {missing_columns}")

            print("Data loaded successfully")
        
        except FileNotFoundError:
            print(f"Error: File not found at {self.data_source}")
        
        except ValueError as e:
            print(f"Error: {e}")
        
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
    
    def load_unionable_tables(self, path):
        #load the mapping between query and its unionnable tables generated by a system like Starmie
        
         self.unionable_data= utl.loadDictionaryFromPickleFile(path)    
    
    def sum_vectors(self, emb_dic, table_name, columns):
        table_vector=np.zeros(768)             
        for _col in columns:
            table_vector=table_vector+emb_dic[table_name][_col]
        return table_vector
    
    def average_vectors(self, emb_dic, table_name, columns):
        table_vector=np.zeros(768) 
        col_number= len(columns)
        table_vector=self.sum_vectors(emb_dic, table_name, columns)
        table_vector=np.divide(table_vector,float(col_number) )
        return    table_vector
            
    def vectorize_tables(self):
        '''we need to represent each table as a point in space so for  I represent each query table and unionable table produced from  their aligned columns 
        '''
        combining_methods={"sum", "average"}
        (q_emb_dict, dl_emb_dict)=self.load_starmie_vectors()
        
        # every query table and its corresponind unionable tables is loaded to  self.data 
        # Columns in data:  required_columns = ['query_table_name', 'query_column', 'query_column#',
        #                         'dl_table_name', 'dl_column#', 'dl_column']
       
        # for every query and s_i pair we get alignments get the columns vector
        # representation and sum their vectors to represent query and data lake tables and write to a new dictionary 
        # key is (query, dl_table) value is (query_vector, dl_table_vector)
       
        
# Define the column names
        columns_query_dl_table_vector_df = ["query", "si", "query_sum_vector", "query_avg_vector", "s_i_sum_vector", "s_i_avg_vector"]

# Create an empty DataFrame
        query_dl_table_vector_df = pd.DataFrame(columns=columns_query_dl_table_vector_df)
        # get the query names 
        query_dl_table_vector_vectors_exists = os.path.isfile('query_dl_table_vector.csv')
        if  query_dl_table_vector_vectors_exists:
            #load 
            with open('query_dl_table_vector.pkl', "rb") as file:
                 self.query_dl_table_vector_df = pickle.load(file)
        else:   
                query_table_name_set = set(self.data['query_table_name'])
                for query in query_table_name_set:
                    
                    # Filter rows where 'query_table_name' column has the value 4
                    filtered_df = self.data[self.data['query_table_name'] == query]

                    # Get the set of all values in column 'W' for the filtered rows
                    dl_table_names= set(filtered_df['dl_table_name'])
                    for dl_table in dl_table_names:
                        # get columns aligned from query 
                        filtered_rows = filtered_df[filtered_df['dl_table_name'] == dl_table]
                        
                        # Extract values from the 'query_column' column
                        query_columns= set(filtered_rows['query_column#'].tolist()  )             
                        # go through the couluns and add their starmie representaiotn 
                        query_table_vector_sum=self.sum_vectors(q_emb_dict,query,query_columns)
                        query_table_vector_avg=self.average_vectors(q_emb_dict,query,query_columns)
                
                
                        dl_table_columns = set(filtered_rows['dl_column'].tolist() )
                        dl_table_vector_sum=self.sum_vectors(dl_emb_dict,dl_table,dl_table_columns)
                        dl_table_vector_avg=self.average_vectors(dl_emb_dict,dl_table,dl_table_columns)
                
                            
                        
                        # Define values for a new row
                        new_row = {
                            "query": query,
                            "si": dl_table,
                            "query_sum_vector": query_table_vector_sum,  # Example vector
                            "query_avg_vector": query_table_vector_avg,  # Example vector
                            "s_i_sum_vector": dl_table_vector_sum,    # Example vector
                            "s_i_avg_vector": dl_table_vector_avg     # Example vector
                        }

                        # Append the new row to the DataFrame
                        self.query_dl_table_vector_df = pd.concat([self.query_dl_table_vector_df, pd.DataFrame([new_row])], ignore_index=True)
                self.query_dl_table_vector_df.to_csv('query_dl_table_vector.csv',columns=query_dl_table_vector_df.columns ,index=False)

                self.query_dl_table_vector_df.to_pickle('query_dl_table_vector.pkl')

        #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
        
        # given a query alignemnt between its paor of uninable  tables are loaded to alignemnt_diversity_df: 
        # Define the required columns
        required_columns = ["q_main_table", "s_i", "s_i_column", "s_i_column#", "s_j_table_name", "s_j_column#", "s_j_column"]
        #required_columns = ["q_main_table", "s_i", "s_i_column#", "s_j_table_name", "s_j_column"]
        alignemnt_diversity_df = pd.read_csv(self.alignment_for_diversity_gmc_file, usecols=required_columns)
        
        
        
        # Define the column names
        columns_query_s_i_s_j_vector = ["query", "s_i", "s_j","s_i_sum_vector", "s_i_avg_vector", "s_j_sum_vector", "s_j_avg_vector"]

# Create an empty DataFrame
        query_s_i_s_j_vector_df = pd.DataFrame(columns=columns_query_s_i_s_j_vector)

        query_s_i_s_j_vector_exists = os.path.isfile('query_s_i_s_j_vector.pkl')
        if  query_s_i_s_j_vector_exists:
            #load 
            with open('query_s_i_s_j_vector.pkl', "rb") as file:
                 self.query_s_i_s_j_vector_df = pickle.load(file)
        
        # get the query names 
        else: 
                query_table_name_set = set(alignemnt_diversity_df['q_main_table'])
                for query in query_table_name_set:  
                    filtered_df = alignemnt_diversity_df[alignemnt_diversity_df['q_main_table'] == query]
                    si_names= set(filtered_df['s_i'])
                    for si in si_names:
                        # get columns aligned from query 
                        filtered_rows = filtered_df[filtered_df['s_i'] == si]
                        sj_tables=set(filtered_rows['s_j_table_name'])            
                        for sj in sj_tables:
                            s_j_part=filtered_rows[filtered_rows['s_j_table_name'] == sj]
                            si_columns = set(s_j_part['s_i_column#'].tolist())
                            sj_columns = set(s_j_part['s_j_column'].tolist())
                        
                            si_vector_sum=self.sum_vectors(dl_emb_dict,si,si_columns)
                            si_vector_avg=self.average_vectors(dl_emb_dict,si,si_columns)

                            sj_vector_sum=self.sum_vectors(dl_emb_dict,sj,sj_columns)
                            sj_vector_avg=self.average_vectors(dl_emb_dict,sj,sj_columns)
                            # get column aligned 

                            # Define values for a new row
                            # = ["query", "s_i", "s_j","s_i_sum_vector", "s_i_avg_vector", "s_j_sum_vector", "s_j_avg_vector"]

                            new_row = {
                            "query": query,
                            "s_i": si,
                            "s_j": sj,  # Example vector
                            "s_i_sum_vector": si_vector_sum,  # Example vector
                            "s_i_avg_vector": si_vector_avg,    # Example vector
                            "s_j_sum_vector": sj_vector_sum,   
                            "s_j_avg_vector": sj_vector_avg    # Example vector 
                            }

                        # Append the new row to the DataFrame
                            self.query_s_i_s_j_vector_df = pd.concat([self.query_s_i_s_j_vector_df, pd.DataFrame([new_row])], ignore_index=True)
                            
                self.query_s_i_s_j_vector_df.to_csv('query_s_i_s_j_vector.csv',columns=query_s_i_s_j_vector_df.columns ,index=False)
                self.query_s_i_s_j_vector_df.to_pickle('query_s_i_s_j_vector.pkl')


        print("table representaion generation is done")

    def execute_search(self):
        """
        Perform the GMC search based on the parameters.
        """
        # retrun k result
        k = self.k
        lmda = 0.7
        all_results={}
        queries=self.unionable_data.keys()
        for query in queries:
              #get the unionable tables for query
              S=self.unionable_data[query]
              (gmc_results, time_took)= self.gmc(S, query,k = k, lmda=lmda)
              all_results[query]=  (gmc_results, time_took)
        return all_results
    
    def filter_results(self, criteria):
        """
        Filters the search results based on specified criteria.

        :param criteria: Dictionary containing filtering rules.
        :return: List of filtered results
        """
        # TODO: Implement filtering logic
        pass

    def export_results(self, output_path):
        """
        Export the search results to a specified location.

        :param output_path: Path to save the results (e.g., file path).
        :return: None
        """
        # TODO: Implement logic to save results
        pass

    def summarize_results(self):
        """
        Provide a summary or statistics of the search results.

        :return: Dictionary with summary details
        """
        # TODO: Implement logic to summarize the results
        pass
    def mmc_compute_div_sum(self, s_i: str, div_dict: dict, R_p: set) -> float:
        total_score = 0
        for s_j in R_p:
            if (s_i , s_j) in div_dict:
                total_score += div_dict[(s_i, s_j)]
            elif (s_j , s_i) in div_dict:
                total_score += div_dict[(s_j, s_i)]
            else: # mean there were no alignment found for this pair    
                  total_score=total_score
        return total_score

    def mmc_compute_div_large(self, s_i: str, div_dict: dict, remaining_s_set : set, max_l : int) -> float:  # max_l should be used as: < max_l ; not <= max_l
        div_l_list = [] # we will use the first l values after sorting this.
        for s_j in remaining_s_set: # the items that are not inserted in R_p yet
            if (s_i, s_j) in div_dict:
                div_l_list.append(div_dict[(s_i, s_j)])
            elif (s_j, s_i) in div_dict:
                div_l_list.append(div_dict[(s_j, s_i)])
            else: # Besat means that there were no alignmnet found for this pair and this pair does not contribute to sum from caller    
                div_l_list.append(0.0)

        # print("Div l: ", div_l_list)
        # print("MAX L", max_l - 1)
        div_l_list = sorted(div_l_list, reverse=True)[:max_l - 1]
        return div_l_list
    
    def mmc(self, s_set: set, lmda : float, k: int, sim_dict: dict, div_dict : dict, R_p: set) -> dict: # s_dict contains query id as key and its embeddings as values.
        all_mmc = dict()
        # print("R_P:", R_p)
        p = len(R_p) - 1
        div_coefficient = lmda / (k - 1)
        for s_i in s_set:
            if s_i in sim_dict.keys():
               sim_term = (1 - lmda) * sim_dict[s_i] # was unionable and found alignment
            else:
                sim_term = (1 - lmda) * 0.0     # was unionable and did not found alignment 
            div_term1 = div_coefficient * self.mmc_compute_div_sum(s_i, div_dict, R_p)
            temp_s=set(s_set) - R_p - {s_i}
            temp=self.mmc_compute_div_large(s_i, div_dict,temp_s , k - p)
            temp_sum=sum(temp)
            div_term2 = div_coefficient * temp_sum
            current_mmc = sim_term + div_term1 + div_term2
            all_mmc[s_i] = current_mmc
        # print("current mmc:", current_mmc)
        # print("all_mmc:", all_mmc)
        return all_mmc
    
    
    # def d_sim(self,s_dict : dict, q_embedding: np.ndarray, metric = "cosine", normalize = False) -> dict:
    def extract_d_sim(self, query_name: str) -> dict:
        """
        Extract the similarity dictionary (sim_dict) based on the unionability DataFrame for a given query name.

        :param query_name: Name of the query (Q_table) to filter the unionability DataFrame.
        :return: A dictionary with DL_table as the key and unionability score as the value.
        """
        if self.unionability_scores is not None:
            # Filter the unionability DataFrame for the given query name
            filtered_df = self.unionability_scores[self.unionability_scores['q_table'] == query_name]
            
            # Convert the filtered DataFrame to a dictionary with DL_table as the key and similarity_score as the value
            sim_dict = dict(zip(filtered_df['dl_table'], filtered_df['similarity_score']))
            
            print(f"Similarity dictionary created for query: {query_name}")
            return sim_dict
        else:
            print("Unionability data is not available. Please calculate unionability first.")
            return {}

    def extract_d_div(self,  query_name: str) -> dict:
        div_dict = dict() 
        if self.diversity_scores is not None:
            filtered_df = self.diversity_scores[self.diversity_scores['q_main_table'] == query_name]
            # since we have generate both (t1, t2) and (t2,t1) in diversity_scores we can use tuple as key 
        div_dict = {
    (key[0].replace("'", ""), key[1].replace("'", "")): value
    for key, value in zip(zip(filtered_df['s_i'], filtered_df['s_j']), filtered_df['diversity_score'])
}
        return div_dict

    def min_div_score(self, s_dict: dict, metric = "cosine", normalize = False) -> float:
        if len(s_dict) == 0:
            return [0]
        min_scores = [] # all possible distances
        for current_s1, current_s1_embedding in s_dict.items():
            for current_s2, current_s2_embedding in s_dict.items():
                if current_s1 != current_s2:
                    if metric == "l1":
                        if normalize == True:
                            max_possible_l1 = 2 * len(current_s1_embedding)
                        else:
                            max_possible_l1 = 1
                        current_sim = np.linalg.norm(current_s1_embedding - current_s2_embedding, ord = 1) / max_possible_l1  # utl.CosineSimilarity(current_s1_embedding, current_s2_embedding)
                    elif metric == "l2":
                        if normalize == True:
                            max_possible_l2 = np.sqrt(2 * len(current_s1_embedding))
                        else:
                            max_possible_l2 = 1
                        current_sim = np.linalg.norm(current_s1_embedding - current_s2_embedding, ord = 2) / max_possible_l2 # utl.CosineSimilarity(current_s1_embedding, current_s2_embedding)
                    else: # metric = cosine
                        if normalize == True:
                            current_sim = 1 - ((utl.CosineSimilarity(current_s1_embedding, current_s2_embedding) + 1 ) / 2)
                        else:
                            current_sim = 1 - utl.CosineSimilarity(current_s1_embedding, current_s2_embedding)
                    min_scores.append(current_sim)
        return min_scores

    def min_mix_div_score(self, s_dict: dict, q_set:set, metric = "cosine", normalize = False) -> float:
        if len(s_dict) == 0:
            return [0]
        min_scores = [] # all possible distances
        for current_s1, current_s1_embedding in s_dict.items():
            for current_s2, current_s2_embedding in s_dict.items():
                if current_s1 in q_set and current_s2 in q_set:
                    continue
                if current_s1 != current_s2:
                    if metric == "l1":
                        if normalize == True:
                            max_possible_l1 = 2 * len(current_s1_embedding)
                        else:
                            max_possible_l1 = 1
                        current_sim = np.linalg.norm(current_s1_embedding - current_s2_embedding, ord = 1) / max_possible_l1  # utl.CosineSimilarity(current_s1_embedding, current_s2_embedding)
                    elif metric == "l2":
                        if normalize == True:
                            max_possible_l2 = np.sqrt(2 * len(current_s1_embedding))
                        else:
                            max_possible_l2 = 1
                        current_sim = np.linalg.norm(current_s1_embedding - current_s2_embedding, ord = 2) / max_possible_l2 # utl.CosineSimilarity(current_s1_embedding, current_s2_embedding)
                    else: # metric = cosine
                        if normalize == True:
                            current_sim = 1 - ((utl.CosineSimilarity(current_s1_embedding, current_s2_embedding) + 1 ) / 2)
                        else:
                            current_sim = 1 - utl.CosineSimilarity(current_s1_embedding, current_s2_embedding)
                    min_scores.append(current_sim)
        return min_scores

    def f_prime(self, s_set: set, lmda : float, k: int, sim_dict: dict, div_dict: dict) -> float:
        if len(s_set) == 0:
            return 0
        total_sim_score = 0
        total_div_score = 0
        for s_i in s_set:
            total_sim_score += sim_dict[s_i]
        for s_i in s_set:
            for s_j in s_set:
                if (s_i, s_j) in div_dict:
                    total_div_score += div_dict[(s_i, s_j)]
                else:
                    total_div_score += div_dict[(s_j, s_i)]
        total_sim_score *= (k-1) * (1 - lmda)
        total_div_score *= 2 * lmda
        return (total_sim_score + total_div_score)

    def avg_div_sim(self, q_name, s_prime):
        'Besat code'
        # compute average of similarity component 
        filtered_unionability_scores = self.unionability_scores[self.unionability_scores['q_table'] == q_name]
        sigma=0
        for s_i in s_prime:
            filtered_row = filtered_unionability_scores[filtered_unionability_scores['dl_table'] == s_i]
            similarity_score_value = filtered_row['similarity_score'].values[0]
            sigma=sigma+similarity_score_value
            
        sim_comp=  sigma/float(len(s_prime)) 
            
        # compute average of div component
            
        sigma=0
        
        for i in range(0, len(s_prime)-1):
            for j in range (i+1, len(s_prime)):
                s_i=s_prime[i]
                s_j=s_prime[j] 
                
                row_1=self.diversity_scores[
                (self.diversity_scores['q_main_table'] == q_name) &  
                (self.diversity_scores['s_i'] == s_i) & (self.diversity_scores['s_j'] == s_j)]
               
                row_2=self.diversity_scores[
                (self.diversity_scores['q_main_table'] == q_name) &  
                (self.diversity_scores['s_i'] == s_j) & (self.diversity_scores['s_j'] == s_i)]
                
                if(row_1.shape[0]==1): 
                    diversity_score = row_1['diversity_score'].values[0]
                elif(row_2.shape[0]==1):    
                    diversity_score = row_2['diversity_score'].values[0]
                else:
                    # this means that the given the query no alignemnt between s_i and s_j has been found by DUST so we assign 0  
                    diversity_score=0
                    #raise ValueError('diversity score file has invalid entry')  

                sigma=sigma+diversity_score
        
        div_comp=sigma/float(len(s_prime)*(len(s_prime)-1))
        
        res= sim_comp+div_comp
        
        return res/2.0
        
    def max_div_sim(self, q_name, s_prime):
        # Besat code
        filtered_unionability_scores = self.unionability_scores[self.unionability_scores['q_table'] == q_name]
        
        filtered_rows_1= filtered_unionability_scores[
    (filtered_unionability_scores['dl_table'].isin(s_prime)) 
        ]
        max_value_sim = filtered_rows_1['similarity_score'].max()
  
        '''Filter all rows those s_i and s_j is in s_prime'''
        filtered_rows_2=self.diversity_scores[self.diversity_scores['q_main_table']==q_name]
 
        filtered_rows = filtered_rows_2[
    (filtered_rows_2['s_i'].isin(s_prime)) &
    (filtered_rows_2['s_j'].isin(s_prime))
]
        max_value_div = filtered_rows['diversity_score'].max()
        overall_max = max(max_value_sim, max_value_div)
        return overall_max

    
    def min_div_sim(self, q_name, s_prime):  
        '''Besat code: ind the minimum score amon all the sim and div scores for a q in S_prime'''
        filtered_unionability_scores = self.unionability_scores[self.unionability_scores['q_table'] == q_name]
        
        filtered_rows_1= filtered_unionability_scores[
    (filtered_unionability_scores['dl_table'].isin(s_prime)) 
        ]
        min_value_sim = filtered_rows_1['similarity_score'].min()
  
        '''Filter all rows those s_i and s_j is in s_prime'''
        filtered_rows_2=self.diversity_scores[self.diversity_scores['q_main_table']==q_name]
 
        filtered_rows = filtered_rows_2[
    (filtered_rows_2['s_i'].isin(s_prime)) &
    (filtered_rows_2['s_j'].isin(s_prime))
]
        min_value_div = filtered_rows['diversity_score'].min()
        overall_min = min(min_value_sim, min_value_div)
        # retrun minumum 
        return overall_min     
    # Besat designed function
    def compute_metrics(self, result):
  
        # Define column names
        columns = ["query_table", "avg_div_sim", "max_div_sim", "min_div_score", "k", "gmc_exec_time_secs"]

# Create an empty DataFrame with the specified columns
        metrics_df = pd.DataFrame(columns=columns)

        # go through result and for every query compute two functions: 
        for q, r in result.items():
            s_prime=r[0]
            time_sec=r[1]
    
            if(q == 'workforce_management_information_a.csv' or q== 'workforce_management_information_b.csv'):
                continue 
            print("Genarating Evaluation for query: "+q)
            avg_div_sim=self.avg_div_sim(q,s_prime)
            max_div_sim=self.max_div_sim(q,s_prime)
            min_div_score=self.min_div_sim(q,s_prime)
            
            # Define the row to add (values must match the column order)
            new_row = {
    "query_table": q,
    "avg_div_sim": avg_div_sim,
    "max_div_sim": max_div_sim, 
    "min_div_score": min_div_score, 
    "k":self.k,
    "gmc_exec_time_secs":time_sec
}

# Add the row to the DataFrame
            metrics_df = pd.concat([metrics_df, pd.DataFrame([new_row])], ignore_index=True)

# Write the DataFrame to a CSV file
        csv_file_path = "evaluation_metrics_gmc.csv"
        
        if os.path.exists(csv_file_path):
        # File exists, append without writing headers
            metrics_df.to_csv(csv_file_path, mode='a', header=False, index=False)
            print(f"Appended data to existing file: {csv_file_path}")
        else:
        # File does not exist, create new file with headers
            metrics_df.to_csv(csv_file_path, mode='w', header=True, index=False)
            print(f"Created new file and wrote data: {csv_file_path}")
        
        return csv_file_path
    
    def compute_metric_old(self, result, dl_embeddings:dict, query_embeddings:dict, lmda:float, k:float, print_results = False, normalize = False, metric = "", max_metric = True):
        computed_metrics = [] # list of dictionaries, each dict is a row in the evaluation dataframe. 
        q = np.mean(list(query_embeddings.values()), axis=0)
        ranking_without_query = {}
        for key in result:
            ranking_without_query[key] = dl_embeddings[key]
        final_ranking_with_query = ranking_without_query.copy()
        for key in query_embeddings:
            final_ranking_with_query[key] = query_embeddings[key]
            dl_embeddings[key] = query_embeddings[key] # we do not need separate data lake embeddings anymore, so merging with query.
        
        R_without_query = set(result)
        R_with_query = set(result).union(set(query_embeddings.keys()))
        if max_metric == True:
            if metric == "" or metric == "cosine":
                # Evaluating max diversity using cosine distance:
                sim_dict = d_sim(dl_embeddings, q, metric="cosine", normalize=normalize) # index the similarity between each item in S and q for once so that we can re-use them.
                div_dict = d_div(dl_embeddings, metric = "cosine", normalize=normalize) # index the diversity between each pair of items in S so that we can re-use them.
                cosine_with_query_max_scores = self.f_prime(R_with_query, lmda, k, sim_dict, div_dict)
                cosine_wo_query_max_scores =  self.f_prime(R_without_query, lmda, k, sim_dict, div_dict)
                
                if print_results == True:
                    print("Evaluating max diversity using cosine distance:")
                    print("max score with query: ", cosine_with_query_max_scores)
                    print("max score without query: ", cosine_wo_query_max_scores)
                    print("\n=================================================\n")

            if metric == "" or metric == "l1":
                # Evaluating max diversity using l1 distance:
                sim_dict = d_sim(dl_embeddings, q, metric="l1", normalize=normalize) # index the similarity between each item in S and q for once so that we can re-use them.
                div_dict = d_div(dl_embeddings, metric = "l1", normalize=normalize) # index the diversity between each pair of items in S so that we can re-use them.
                l1_with_query_max_scores = f_prime(R_with_query, lmda, k, sim_dict, div_dict)
                l1_wo_query_max_scores = f_prime(R_without_query, lmda, k, sim_dict, div_dict)
                
                if print_results == True:
                    print("Evaluating max diversity using l1 distance:")
                    print("max score with query: ", l1_with_query_max_scores)
                    print("max score without query: ", l1_wo_query_max_scores)
                    print("\n=================================================\n")
            if metric == "" or metric == "l2":
                # Evaluating max diversity using l2 distance:
                sim_dict = d_sim(dl_embeddings, q, metric="l2", normalize=normalize) # index the similarity between each item in S and q for once so that we can re-use them.
                div_dict = d_div(dl_embeddings, metric = "l2", normalize=normalize) # index the diversity between each pair of items in S so that we can re-use them.
                l2_with_query_max_scores = f_prime(R_with_query, lmda, k, sim_dict, div_dict)
                l2_wo_query_max_scores = f_prime(R_without_query, lmda, k, sim_dict, div_dict)            
            if print_results == True:
                print("Evaluating max diversity using l2 distance:")
                print("max score with query: ", l2_with_query_max_scores)
                print("max score without query: ", l2_wo_query_max_scores)
                print("\n=================================================\n")
        else:
            cosine_with_query_max_scores = np.nan
            cosine_wo_query_max_scores = np.nan
            l1_with_query_max_scores = np.nan
            l1_wo_query_max_scores = np.nan
            l2_with_query_max_scores = np.nan
            l2_wo_query_max_scores = np.nan
        if metric == "" or metric == "cosine":    
            # Evaluating max-min diversity and average distance using cosine distance:
            cosine_with_query_min_scores = min_div_score(final_ranking_with_query, metric="cosine", normalize=normalize)
            cosine_wo_query_min_scores = min_div_score(ranking_without_query, metric = "cosine", normalize=normalize)
            cosine_with_query_avg_scores =  sum(cosine_with_query_min_scores) / len(cosine_with_query_min_scores)
            cosine_wo_query_avg_scores = sum(cosine_wo_query_min_scores) / len(cosine_wo_query_min_scores)
            
            # The function below computes distance between pairs of data lake and data lake points, data lake and query points but not query and query points. 
            cosine_w_mix_query_min_scores = min_mix_div_score(final_ranking_with_query, set(query_embeddings.keys()), metric="cosine", normalize=normalize)
            cosine_w_mix_query_avg_scores =  sum(cosine_w_mix_query_min_scores) / len(cosine_w_mix_query_min_scores)

            if print_results == True:
                print ("Evaluating max-min diversity and average distance using cosine distance:")
                print("max-min score with query: ", min(cosine_with_query_min_scores))
                print("max-min score without query: ", min(cosine_wo_query_min_scores))
                print("max-min score with mix query: ", min(cosine_w_mix_query_min_scores))
                print("average distance with query: ", cosine_with_query_avg_scores)
                print("average distance without query: ", cosine_wo_query_avg_scores)
                print("average distance with mix query: ", cosine_w_mix_query_avg_scores)
                print("\n=================================================\n")
        if metric == "" or metric == "l1":
            # Evaluating max-min diversity and average distance using l1 distance:
            l1_with_query_min_scores = min_div_score(final_ranking_with_query, metric="l1", normalize=normalize)
            l1_wo_query_min_scores = min_div_score(ranking_without_query, metric = "l1", normalize=normalize)
            l1_with_query_avg_scores = sum(l1_with_query_min_scores) / len(l1_with_query_min_scores)
            l1_wo_query_avg_scores = sum(l1_wo_query_min_scores) / len(l1_wo_query_min_scores)
            
            # The function below computes distance between pairs of data lake and data lake points, data lake and query points but not query and query points. 
            l1_w_mix_query_min_scores = min_mix_div_score(final_ranking_with_query, set(query_embeddings.keys()), metric="l1", normalize=normalize)
            l1_w_mix_query_avg_scores =  sum(l1_w_mix_query_min_scores) / len(l1_w_mix_query_min_scores)

            if print_results == True:
                print ("Evaluating max-min diversity and average distance using l1 distance:")
                print("max-min score with query: ", min(l1_with_query_min_scores))
                print("max-min score without query: ", min(l1_wo_query_min_scores))
                print("max-min score with mix query: ", min(l1_w_mix_query_min_scores))
                print("average distance with query: ", l1_with_query_avg_scores)
                print("average distance without query: ", l1_wo_query_avg_scores)
                print("average distance with mix query: ", l1_w_mix_query_avg_scores)
                print("\n=================================================\n")
        
        if metric == "" or metric == "l2":
            # Evaluating max-min diversity and average distance using l2 distance:
            l2_with_query_min_scores = min_div_score(final_ranking_with_query, metric= "l2", normalize=normalize)
            l2_wo_query_min_scores = min_div_score(ranking_without_query, metric = "l2", normalize=normalize)
            l2_with_query_avg_scores = sum(l2_with_query_min_scores) / len(l2_with_query_min_scores)
            l2_wo_query_avg_scores = sum(l2_wo_query_min_scores) / len(l2_wo_query_min_scores)
            
            # The function below computes distance between pairs of data lake and data lake points, data lake and query points but not query and query points. 
            l2_w_mix_query_min_scores = min_mix_div_score(final_ranking_with_query, set(query_embeddings.keys()), metric="l2", normalize=normalize)
            l2_w_mix_query_avg_scores =  sum(l2_w_mix_query_min_scores) / len(l2_w_mix_query_min_scores)

            if print_results == True:
                print("Evaluating max-min diversity and average distance using l2 distance:")
                print("score with query: ", min(l2_with_query_min_scores))
                print("score without query: ", min(l2_with_query_min_scores))
                print("max-min score with mix query: ", min(l2_w_mix_query_min_scores))
                print("average distance with query: ", l2_with_query_avg_scores)
                print("average distance without query: ", l2_wo_query_avg_scores)
                print("average distance with mix query: ", l2_w_mix_query_avg_scores)
                print("\n=================================================\n")
        
        # create 6 dictionaries to store all the calculations
        if metric == "" or metric == "cosine": 
            computed_metrics.append({"metric": "cosine", "with_query" : "yes", "max_score": cosine_with_query_max_scores, "max-min_score": min(cosine_with_query_min_scores), "avg_score": cosine_with_query_avg_scores})
            computed_metrics.append({"metric": "cosine", "with_query": "no", "max_score": cosine_wo_query_max_scores, "max-min_score": min(cosine_wo_query_min_scores), "avg_score": cosine_wo_query_avg_scores})
            computed_metrics.append({"metric": "cosine", "with_query": "mix", "max_score": np.nan, "max-min_score": min(cosine_w_mix_query_min_scores), "avg_score": cosine_w_mix_query_avg_scores})
            

        if metric == "" or metric == "l1":
            computed_metrics.append({"metric": "l1", "with_query" : "yes", "max_score": l1_with_query_max_scores, "max-min_score": min(l1_with_query_min_scores), "avg_score": l1_with_query_avg_scores})
            computed_metrics.append({"metric": "l1", "with_query": "no", "max_score": l1_wo_query_max_scores, "max-min_score": min(l1_wo_query_min_scores), "avg_score": l1_wo_query_avg_scores})
            computed_metrics.append({"metric": "l1", "with_query": "mix", "max_score": np.nan, "max-min_score": min(l1_w_mix_query_min_scores), "avg_score": l1_w_mix_query_avg_scores})

        if metric == "" or metric == "l2":
            computed_metrics.append({"metric": "l2", "with_query" : "yes", "max_score": l2_with_query_max_scores, "max-min_score": min(l2_with_query_min_scores), "avg_score": l2_with_query_avg_scores})
            computed_metrics.append({"metric": "l2", "with_query": "no", "max_score": l2_wo_query_max_scores, "max-min_score": min(l2_wo_query_min_scores), "avg_score": l2_wo_query_avg_scores})
            computed_metrics.append({"metric": "l2", "with_query": "mix", "max_score": np.nan, "max-min_score": min(l2_w_mix_query_min_scores), "avg_score": l2_w_mix_query_avg_scores})

        return computed_metrics, embedding_plot
    

    # def gmc(self, S_dict: dict, q_dict:dict, k: int, lmda: float = 0.7, metric = "cosine", print_results = False, normalize = False, max_metric = True, compute_metric = True) -> set: #S_dict is a dictionary with tuple id as key and its embeddings as value. 
    def gmc(self, S_names: set, query_name:str, k: int, lmda: float = 0.7, metric = "cosine", print_results = False, normalize = False, max_metric = True, compute_metric = True) -> set: #S_dict is a dictionary with tuple id as key and its embeddings as value. 

        '''adopted from https://anonymous.4open.science/r/dust-B79B/diversity_algorithms/div_utilities.py'''
    #the metric is for sim dict and div dict, and is independent of evaluation. we evaluate using all 
    # three metrics and in compute_metric() function, we again compute sim_dict and div_dict.
        #Besat:load starmie embeddings for computin metrics as we have used thoes embeding in diversity score
        (q_emb_dict, dl_emb_dict)=self.load_starmie_vectors()
        #Besat since in diversity and similarity score computations I added the values for each column 
        # that are aligned for evaluting the diversity metrics I decided to represent each table as sum of their aligned vectors
        
        start_time = time.time_ns()
        # q = np.mean(list(q_dict.values()), axis=0)
        R = set()
        ranked_div_result = []
        sim_dict = self.extract_d_sim(query_name) # index the similarity between each item in S and q for once so that we can re-use them.
        div_dict = self.extract_d_div(query_name) # index the diversity between each pair of items in S so that we can re-use them.
        # debug_dict(sim_dict, 5, "sim dict")
        # debug_dict(div_dict, 5, "div dict")
        S_set = S_names
        for p in range(0, k):
            if len(S_set) == 0:
                break
            mmc_dict = self.mmc(S_set, lmda, k, sim_dict, div_dict, R) # send S to mmc and compute MMC for each si
            s_i  = max(mmc_dict, key=lambda k: mmc_dict[k])
            R.add(s_i)
            ranked_div_result.append(s_i)
            S_set = set(S_set) - {s_i}
        # print("GMC f score:", f_prime(R, lmda, k, sim_dict, div_dict))
        end_time = time.time_ns()
        total_time = round(int(end_time - start_time) / 10 ** 9, 2)
        print("Total time taken: ", total_time, " seconds.")
        
        return ranked_div_result,total_time
        
    def _cosine_sim(self, vec1, vec2):
        ''' Get the cosine similarity of two input vectors: vec1 and vec2
        '''
        assert vec1.ndim == vec2.ndim
        return np.dot(vec1, vec2) / (norm(vec1)*norm(vec2))
    
    
    
    def load_starmie_vectors(self):
        '''load starmie vectors for query and data lake and retrun as dictionaries'''
        dl_table_vectors = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/data/santos/vectors/cl_datalake_drop_col_tfidf_entity_column_0.pkl"
        query_table_vectors = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/data/santos/vectors/cl_query_drop_col_tfidf_entity_column_0.pkl"
        qfile = open(query_table_vectors,"rb")
         # queries is a list of tuples ; tuple of (str(filename), numpy.ndarray(vectors(numpy.ndarray) for columns) 
        queries = pickle.load(qfile)
        # make as dictnary from first item to secon item 
        # Convert to dictionary
        queries_dict = {item[0]: item[1] for item in queries}

        tfile = open(dl_table_vectors,"rb")
        tables = pickle.load(tfile)
        # tables is a list of tuples ; tuple of (str(filename), numpy.ndarray(vectors(each verstor is a numpy.ndarray) for columns) 
    
        dl_dict = {item[0]: item[1] for item in tables}
        
        return (queries_dict,dl_dict)
    
    def calculate_unionability(self):
        ''' aligned columns are loaded in self.data
            get the vector represenation from starmie 
            calulate the cosine similarity 
            avg col sims to get table similarity and this is unionability  or similaryty score and then write result in the file 
        '''
        all_vectors=self.load_starmie_vectors()
        queries_dict = all_vectors[0]
        dl_dict=all_vectors[1]

        sim_data = pd.DataFrame(columns=["q_table", "dl_table", "similarity_score"])

    
        # self.data is a datafram with columns ['query_table_name', 'query_column', 'query_column#','dl_table_name', 'dl_column#', 'dl_column']
        
        # get all existing query tables from self.data 
        
        q_table_names = self.data['query_table_name'].unique()
        # for every query now compute the similarity scores with dt tables 
        for query_name in q_table_names:
                 # get q columns vectors 
                q_vectors=queries_dict[query_name]
                # Get all rows corresponding to the current query_name
                query_rows = self.data[self.data['query_table_name'] == query_name]

                # Get all unique dl_table_names for the current query_name
                dl_table_names = query_rows['dl_table_name'].unique()
                    # Iterate over each dl_table_name
                for dl_table_name in dl_table_names:
                    similarity_score=0
                    # Filter rows for the current query_name and dl_table_name
                    specific_rows = query_rows[query_rows['dl_table_name'] == dl_table_name]
                    dl_t_vectors=dl_dict[dl_table_name]
                    # Retrieve the relevant columns
                    num_rows = float(specific_rows.shape[0])

                    for _, row in specific_rows.iterrows():
                         # get their vectors 
                        query_column = row['query_column#']
                        dl_column = row['dl_column']
                        # Call the similarity function
                        similarity_col = self._cosine_sim(q_vectors[query_column],dl_t_vectors[dl_column])
                        similarity_score=similarity_col+similarity_score

                    similarity_score=similarity_score/num_rows
                    # Add a row with the current q_table, dl_table, and similarity_score
                    sim_data = pd.concat([
                        sim_data,
                        pd.DataFrame({"q_table": [query_name], "dl_table": [dl_table_name], "similarity_score": [similarity_score]})
                    ], ignore_index=True) 
                    
        return   sim_data         
   
   
    def item_frequency(self, lst):
        return dict(Counter(lst))
    
   
    def   Jensen_Shannon_distances(self,query_column,dl_column,domain_estimate):
        # build the x axis for both columns converting the  domain_estimate to a set of tuples
                        # each tuple <item label, item index in x axis>
            x_axis={}
            i=0
            for item in domain_estimate:
                x_axis[item]=i
                i=i+1
            
            #now build the probability array   
            frequency_q= self.item_frequency(query_column)
            frequency_dl= self.item_frequency(dl_column)
            
            list_length_q=len(query_column)
            list_length_dl=len(dl_column)
            
            
            #probability arrays
            array_q  = np.zeros(len(domain_estimate)) 
            array_dl = np.zeros(len(domain_estimate)) 
            
            for item in domain_estimate:
                index_= x_axis[item]
                if(item in frequency_q):
                    freq_q_item= frequency_q[item]
                    array_q[index_]=freq_q_item/float(list_length_q)
                else: 
                      array_q[index_]=0
                if (item in frequency_dl):   
                    freq_dl_item= frequency_dl[item]
                    array_dl[index_]=freq_dl_item/float(list_length_dl)
                else: 
                    array_dl[index_]=0    
                
            #The Jensen-Shannon distances
            dis_qdl=distance.jensenshannon(array_q,array_dl) 
            dis_dlq=distance.jensenshannon(array_dl,array_q)     
            if(dis_qdl!=dis_dlq):
                    raise ValueError('the distance metric is asymmetric')  
            else:  
                    return dis_dlq  
   
    def _lexicalsim_Pair(self, query_column, table_column):
    # The input sets must be a Python list of iterables (i.e., lists or sets).
        sets = [query_column, set(table_column)]
        #sets = [[1,2,3], [3,4,5], [2,3,4], [5,6,7]]

        # all_pairs returns an iterable of tuples.
        pairs = all_pairs(sets, similarity_func_name="jaccard", similarity_threshold=0.0)
        l_pairs=list(pairs)
        # [(1, 0, 0.2), (2, 0, 0.5), (2, 1, 0.5), (3, 1, 0.2)]
        # Each tuple is (<index of the first set>, <index of the second set>, <similarity>).
        # The indexes are the list indexes of the input sets.
        if len(l_pairs)==0:
            return 0
        else:
            return l_pairs[0][2]
    
    
    def calculate_diversity(self, precalculated_file):
        
        """
           diversity between two tables are the average of diversity between theri columns for now 
         """
        #load required data here 
        dataFolder="santos"
        table_path = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/data/santos/vectors/cl_datalake_drop_col_tfidf_entity_column_0.pkl"
        query_path_raw = "data/"+dataFolder+"/"+"query"
        table_path_raw = "data/"+dataFolder+"/"+"datalake"
        index_path="data/indices/"
        processed_path="data/processed/"+dataFolder+"/"
        index_file_path="/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/data/indices/Joise_Index_DL_santos_tokenized_bot.pkl"
        
        
        text_processor = TextProcessor()

        # we preprocess the values in tables both query and data lake tables
        list_of_lists=[]
        
        self.tables_raw=NaiveSearcherNovelty.read_csv_files_to_dict(table_path_raw)
        
        #the normalized/tokenized/original with no duplicates  dl(data lake) tables are stored in table_raw_proccessed_los
        table_raw_proccessed_los={}
        
            # write the proccessed result having columns as set to a pickle file 
        dl_tbls_processed_set_file_name="dl_tbls_processed_set.pkl"
        
        
        table_raw_proccessed_los=test_naive_search_Novelty.getProcessedTables(text_processor, dl_tbls_processed_set_file_name, processed_path, self.tables_raw,"los", 1, 1)
        # process dl tables and save as list of lists 
        self.table_raw_proccessed_los=table_raw_proccessed_los
        table_raw_lol_proccessed={}
            # write the proccessed result having columns as set to a pickle file 
        dl_tbls_processed_lol_file_name="dl_tbls_processed_lol.pkl"
        self.dl_tbls_processed_lol_file_name=dl_tbls_processed_lol_file_name
        table_raw_lol_proccessed=test_naive_search_Novelty.getProcessedTables(text_processor,  dl_tbls_processed_lol_file_name,processed_path,self. tables_raw,"lol", 1, 1)
        self.table_raw_lol_proccessed=table_raw_lol_proccessed
     
        table_raw_index={}

       
        index_exists = os.path.isfile(index_file_path)
        if index_exists:
        #load it  
            print("loading Joise Index......")
            with open(index_file_path, 'rb') as file:
                table_raw_index = pickle.load(file)
        else:    
            
            for key, value in table_raw_proccessed_los.items():
                
                index = SearchIndex(value, similarity_func_name="jaccard", similarity_threshold=0.0)
                table_raw_index[key]= index   
                
            # write in a pickle file  
            with open(index_file_path, 'wb') as file:
                    pickle.dump(table_raw_index, file)   
        
        
        self.table_raw_index=table_raw_index
        self.table_path=table_path
        #DSize is a hyper parameter
        DSize=20
    
        # load the diversity data from disk having aligned columns
        # columns: (["q_main_table","s_i", "s_i_column", "s_i_column#", "s_j_table_name", "s_j_column#","s_j_column"])
        diversity_path=precalculated_file
        # THERE ARE \r character in some rows 
        
        
        # Define the required columns
        required_columns = ["q_main_table", "s_i", "s_i_column", "s_i_column#", "s_j_table_name", "s_j_column#", "s_j_column"]
        #required_columns = ["q_main_table", "s_i", "s_i_column#", "s_j_table_name", "s_j_column"]

        try:
            # Load the CSV file into a pandas DataFrame
            diversity_df = pd.read_csv(diversity_path, usecols=required_columns)
            
            # Display a success message and the DataFrame's head
            print("CSV file loaded successfully.")
           # print(diversity_df.head())

        except FileNotFoundError:
            print(f"Error: File not found at {diversity_path}")
        except ValueError as e:
            print(f"Error: {e}")

        df_diversity_score= pd.DataFrame(columns=["q_main_table", "s_i", "s_j", "diversity_score"])

        q_main_table_names = diversity_df['q_main_table'].unique()
        # for every query now compute the similarity scores with dt tables 
        for query_name in q_main_table_names:
                query_rows = diversity_df[diversity_df['q_main_table'] == query_name]

                # Get all unique s_i_names for the current query_name
                s_i_names = query_rows['s_i'].unique()
                    # Iterate over each dl_table_name
                for s_i in s_i_names:
                    s_j_rows= query_rows[(query_rows['q_main_table'] == query_name) & (query_rows['s_i'] == s_i)]
                    s_j_names = s_j_rows['s_j_table_name'].unique()
                    div_score=0
                    for s_j in s_j_names:
                        s_i_s_j_rows= s_j_rows[(s_j_rows['q_main_table'] == query_name) & (s_j_rows['s_i'] == s_i) & (s_j_rows['s_j_table_name'] == s_j)]
                        distance=0
                        div_score=0
                        num_rows = float(s_i_s_j_rows.shape[0])
                        for _, row in s_i_s_j_rows.iterrows():
                            s_i_column_number = int(row['s_i_column#'])
                            s_j_column_number = int(row['s_j_column'])
                            
                            # compute the diversity  for columns
                            
                            #get the comlumn from data lake table
                            dl_column_s_i=self.table_raw_lol_proccessed.get(s_i)[s_i_column_number]
                            dl_column_set_s_i=self.table_raw_proccessed_los.get(s_i)[s_i_column_number]
                            
                            #get the comlumn from data lake table
                            dl_column_s_j=self.table_raw_lol_proccessed.get(s_j)[s_j_column_number]
                            dl_column_set_s_j=self.table_raw_proccessed_los.get(s_j)[s_j_column_number]

                        
                            #see what is the number of unique values in the query+ dl columns that are list of list 
                            # we have a threshold to determine the smallness of domain called DS(domain size)
                            # Besat to change: here we do not merge tokens from all cell to 
                            # gether for each column maybe this will change later ?
                            domain_estimate=set.union(set(dl_column_s_j),set(dl_column_s_i) )
                            if(len(domain_estimate)<DSize):
                                distance=self.Jensen_Shannon_distances(dl_column_s_i,dl_column_s_j,domain_estimate)
                                # log the domian infomrmation
                            else: 
                                # jaccard distance
                                distance=1-self._lexicalsim_Pair(dl_column_set_s_i,dl_column_set_s_j)
                            div_score=div_score+distance
                                        
                                                            
                        # now compute the average         
                        div_score=div_score/num_rows        
                        new_row = {
                            "q_main_table":query_name ,
                            "s_i": s_i,
                            "s_j": s_j,
                            "diversity_score": div_score
                        }


                        # Convert the new row to a DataFrame
                        new_row_df = pd.DataFrame([new_row])

                        # Concatenate the new row with the existing DataFrame
                        df_diversity_score = pd.concat([df_diversity_score, new_row_df], ignore_index=True)
                                                # Append the new row to the DataFrame
                    

        return df_diversity_score
      
                   
    def generate_alignment_for_diversity(self, file_path):
        
        if self.data is not None:
                try:
                    # Initialize an empty list to store the results
                    diversity_pairs = []

                    # Group the data by Q_table
                    grouped = self.data.groupby('query_table_name')

                    # Iterate over each group
                    for q_table, group in grouped:
                        # Further group by DL_table within each Q_table group
                        dl_grouped = group.groupby('dl_table_name')
                        
                        for dl_table, dl_group in dl_grouped:
                            # Get the set of DL_columns for this Q_table and DL_table
                            dl_columns_set = set(dl_group['dl_column'])

                            # Append the triple to the results
                            diversity_pairs.append((q_table, dl_table, dl_columns_set))
                    
                    print("Diversity pairing complete.")

                except KeyError as e:
                    print(f"Error: Missing required column in data - {e}")
                    return None
                except Exception as e:
                    print(f"An unexpected error occurred: {e}")
                    return None
            

                if os.path.exists(file_path):
                        print(f"Loaded existing diversity scores from {file_path}")
                else:    # Assuming `diversity_pairs` is a list of triples (Q_table, DL_table, set of DL_columns)
                        if diversity_pairs:
                            for q_main_table, s_i, dl_columns in diversity_pairs:
                                print(f"Processing Q_table: {q_main_table}, s_i: {s_i}, Columns: {dl_columns}")

                                # Call get_diversity_by_alignment for each row and get  a  data frames  with multiple rows 
                                # corresponding to alignments 
                                S_j= self.data[self.data["query_table_name"] == q_main_table]["dl_table_name"].unique()
                                res= self.export_alignment_for_diversity(q_main_table, s_i,S_j, dl_columns,file_path)
        return res                        
                                
                                
    def export_alignment_for_diversity(self, q_main_table,s_i,S_j, columns_,output_csv_path):
        """
        get  diversity data for each DL_table and s_i pair 
        however  limit s_i to its columns_ then find alignment with each of 50 unionable dl_tables
                

        :return: A pandas DataFrame with the diversity data for s_i and columns_ 
        """


        alignment_result=gmc_alignmnet_by_query(s_i,columns_,S_j)
        
        
        output_rows = []
        if not alignment_result is None:
            for row in alignment_result:
            # Prepend q_main_table to each row
                row.insert(0, q_main_table)
                output_rows.append(row)

            # Write results to a CSV file
            try:
                file_exists = os.path.exists(output_csv_path)
                with open(output_csv_path, mode="a", newline="") as csvfile:
                    csv_writer = csv.writer(csvfile)
                    # Write header if file does not exist
                    if not file_exists:
                        csv_writer.writerow(["q_main_table","s_i", "s_i_column", "s_i_column#", "s_j_table_name", "s_j_column#","s_j_column"])
                    # Write rows
                    csv_writer.writerows(output_rows)

                print(f"Alignment results written successfully to {output_csv_path} for query {q_main_table}" )
                return True
            except Exception as e:
                print(f"Error writing alignment results to CSV: {e}")
                return False
        else:
            return True
        
    def clean_file(self):
        import re

        # Define the input and output file paths
        input_file = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/alignment_for_diversity_gmc_Santos.csv"
        output_file = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/alignment_for_diversity_gmc_Santos_cleaned.csv"
        # input_file = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/alignment_for_diversity_gmc_Santos_small.csv"
        # output_file = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/alignment_for_diversity_gmc_Santos_small_cleaned.csv"
    
      
        file_exists = os.path.exists(output_file)
        if not file_exists:
                try:
                    # Open the CSV file as text
                    with open(input_file, "r", encoding="utf-8") as file:
                        text = file.read()
                # Regex pattern to match substrings starting with a newline, then double-quote and comma
                    pattern1 = r'\n,'
                    replacement1 = ','
                    pattern2 = r',"'
                    # Replacement string
                    replacement2 = ','
                    # Perform the replacement
                    output_text = re.sub(pattern1, replacement1, text)
                    # output_text = re.sub(pattern2, replacement2, output_text)
                    # Write the modified text into a new file
                    with open(output_file, "w", encoding="utf-8") as file:
                        file.write(output_text)

                    print(f"Successfully replaced '\\n' and wrote to {output_file}.")
                    return output_file
                except FileNotFoundError:
                    print(f"Error: The file {input_file} was not found.")
                except Exception as e:
                    print(f"An error occurred: {e}")    
        else:        
            print("file is cleaned")
            return output_file
 

    
def d_sim(self,s_dict : dict, q_embedding: np.ndarray, metric = "cosine", normalize = False) -> dict:
    sim_dict = dict() # key: s_dict key i.e. s_id; value : similarity score
    for current_s, current_s_embedding in s_dict.items():
        if metric == "l1":
            if normalize == True:
                max_possible_l1 = 2 * len(current_s_embedding)
            else:
                max_possible_l1 = 1
            current_sim = np.linalg.norm(current_s_embedding - q_embedding, ord = 1) / max_possible_l1
        elif metric == "l2":
            if normalize == True:
                max_possible_l2 = np.sqrt(2 * len(current_s_embedding))
            else:
                max_possible_l2 = 1
            current_sim = np.linalg.norm(current_s_embedding - q_embedding, ord = 2) / max_possible_l2
        else: # cosine
            if normalize == True:
                current_sim = 1 - ((utl.CosineSimilarity(current_s_embedding, q_embedding) + 1 ) / 2)
            else:
                current_sim = 1 - utl.CosineSimilarity(current_s_embedding, q_embedding)
        sim_dict[current_s] = current_sim
    return sim_dict

def d_div(self,s_dict : dict, metric = "cosine", normalize = False) -> dict:
    div_dict = dict() # key: s_dict key i.e. s_id; value : similarity score
    for current_s1 in s_dict:
        for current_s2 in s_dict:
            if metric == "l1":
                max_possible_l1 = 2 * len(s_dict[current_s1])
                if normalize == True:
                    current_div = 1 - (np.linalg.norm(s_dict[current_s1] - s_dict[current_s2], ord = 1) / max_possible_l1)
                else:
                    current_div = max_possible_l1 - (np.linalg.norm(s_dict[current_s1] - s_dict[current_s2], ord = 1) / 1)                
            elif metric == "l2":
                max_possible_l2 = np.sqrt(2 * len(s_dict[current_s1]))
                if normalize == True:
                    current_div = 1 - (np.linalg.norm(s_dict[current_s1] - s_dict[current_s2], ord = 2) / max_possible_l2)
                else:
                    current_div = max_possible_l2 - (np.linalg.norm(s_dict[current_s1] - s_dict[current_s2], ord = 2) / 1)
            else: #cosine
                if normalize == True:
                    current_div = (utl.CosineSimilarity(s_dict[current_s1], s_dict[current_s2]) + 1) / 2 #normalized score between 0 and 1
                else:
                    current_div = utl.CosineSimilarity(s_dict[current_s1], s_dict[current_s2])
            div_dict[(current_s1, current_s2)] = current_div
    return div_dict

      
if __name__ == "__main__":
    # Example usage:
    alignment_Dust="/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/DUST_Alignment_Santos.csv"
    
    alignment_for_diversity_gmc_file='/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/alignment_for_diversity_gmc_Santos.csv'

    first_50_starmie="/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/groundtruth/santos_union_groundtruth.pickle"    
    
    
    
    # input_file = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/alignment_for_diversity_gmc_Santos_cleaned.csv"
    # output_file = "/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/partial.csv"

    # # Load the CSV file into a DataFrame
    # df = pd.read_csv(input_file)

    # # Filter the DataFrame based on the given conditions
    # filtered_df = df[
    #     (df['q_main_table'] == 'biodiversity_a.csv') & 
    #     ((df['s_i'] == 'biodiversity_4.csv') | (df['s_j_table_name'] == 'biodiversity_4.csv')) & 
    #     ((df['s_i'] == 'tuition_assistance_program_tap_recipients_a.csv') | (df['s_j_table_name'] == 'tuition_assistance_program_tap_recipients_a.csv'))
    # ]

    # # Write the filtered rows into a new CSV file
    # filtered_df.to_csv(output_file, index=False)

    # print(f"Filtered rows have been written to {output_file}")
    
    
    
    
    

    
    search_params = {"keyword": "example", "max_results": 10}

    gmc_search = GMC_Search(alignment_Dust, search_params)

    # we leaod the data corresponindt to acolumn alignment generated by DUST for the output of
    # Starmie on Santos returning maximum 50 unionable dl tables  for each query
    gmc_search.load_data()
    gmc_search.load_unionable_tables(first_50_starmie)
    
    #generate and persist alignments for diversity function 
    # first check if the file exist if not creat it by calling function 
    file_exists = os.path.exists(alignment_for_diversity_gmc_file)
    if file_exists:
        print("alignments for diversity file exists")
    else:
        initialize_globally() # for DUST 
        gmc_search.generate_alignment_for_diversity( alignment_for_diversity_gmc_file)
            
  
    gmc_search.alignment_for_diversity_gmc_file=gmc_search.clean_file()   
    # Calculate unionability

    uionability_file_path="/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/gmc_unionability_scores.csv"
    file_exists = os.path.exists(uionability_file_path)
    if not file_exists:
        unionability_scores = gmc_search.calculate_unionability()

        unionability_scores.to_csv(
            uionability_file_path,
            index=False,  # Do not include the index in the CSV
            columns=unionability_scores.columns,  # Use DataFrame columns for the CSV
            sep=",",  # Use a comma as the separator
            quotechar='"',  # Handle quoted strings
            encoding="utf-8",  # Set encoding
        )
    else: 
            unionability_scores= pd.read_csv(
            uionability_file_path,
            sep=",",          # Use a comma as the delimiter
            header=0)        # Treat the first row as the header (column names        )
        
    gmc_search.unionability_scores=unionability_scores
    
    
    diveristy_file_path="/Users/besatkassaie/Work/Research/DataLakes/TableUnionSearch/NAUS/diversity_data/gmc_diversity_scores.csv"
    file_exists = os.path.exists(diveristy_file_path)
    if not file_exists:
# Save DataFrame to CSV
     
        diversity_scores = gmc_search.calculate_diversity(gmc_search.alignment_for_diversity_gmc_file)

        diversity_scores.to_csv(
            diveristy_file_path,
            index=False,  # Do not include the index in the CSV
            columns=diversity_scores.columns,  # Use DataFrame columns for the CSV
            sep=",",  # Use a comma as the separator
            quotechar='"',  # Handle quoted strings
            encoding="utf-8",  # Set encoding
        )
    else: 
         diversity_scores= pd.read_csv(
            diveristy_file_path,
            sep=",",          # Use a comma as the delimiter
            header=0) 
           
    gmc_search.diversity_scores=diversity_scores  

    # create the represenation for tables 
    #gmc_search.vectorize_tables()
    for i in range(2, 11): 
            gmc_search.k=i

            results = gmc_search.execute_search()
            # Define the output CSV file path
            output_csv_file = 'gmc_results.csv'
            

        # Write the dictionary to a CSV file
            if os.path.exists(output_csv_file):
                with open(output_csv_file, mode='a', newline='') as file:
                    writer = csv.writer(file)
                    # Write the data
                    for query_name, (result, _) in results.items():
                        # Join the list of results into a string, if needed
                        result_str = ', '.join(result) if isinstance(result, list) else str(result)
                        writer.writerow([query_name, result_str,gmc_search.k])
            else: 
                with open(output_csv_file, mode='w', newline='') as file:
                    writer = csv.writer(file)
                    writer.writerow(['query_name', 'tables','k'])

                    # Write the data
                    for query_name, (result, _) in results.items():
                        # Join the list of results into a string, if needed
                        result_str = ', '.join(result) if isinstance(result, list) else str(result)
                        writer.writerow([query_name, result_str,gmc_search.k])
                    # Write the header

            

            
            file=gmc_search.compute_metrics(results)
            
            # evaluate the results 




